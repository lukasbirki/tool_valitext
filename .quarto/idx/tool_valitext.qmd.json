{"title":"ValiText - A Validation Framework for Text-Based Measures of Social Constructs","markdown":{"yaml":{"title":"ValiText - A Validation Framework for Text-Based Measures of Social Constructs","author":"Lukas Birkenmaier","bibliography":"references.bib","format":"html","editor":"visual"},"headingText":"1. Introduction","containsRefs":false,"markdown":"\n\n\nIn this tutorial, we introduce the [ValiText](https://www.valitex.info/) tool, a framework and instrument for validating text-based measures of social constructs.\nValiText helps researchers to execute and document validation by providing conceptual and practical guidance for researchers [@birkenmaier2023valitext].\n\nValidation is a necessary requirement for any text analysis [@grimmer2013text].\nEssentially, the purpose of validation is to ensure that what is to be measured (i.e., the numerical scores assigned to texts) correspond to the true^[Nevertheless, it’s important to acknowledge that the \"true\" nature of a text is inherently unobservable and can only be approximated. For instance, while we might interpret a certain text as positive or negative, these characteristics are not intrinsic to the text itself; they are inferred and open to subjective interpretation [@krippendorff2009content]] nature of the construct being studied.\n\nData Quality is an important requirement for validity. Low quality data can significantly affect the validity of text-based measures.\nFor instance, when the text data is incomplete, ambiguous, or misrepresentative, it becomes challenging to draw accurate inferences about social science phenomena. However, detecting these data quality issues can be difficult. ValiText helps researchers with a shared vocabulary denoting different validation steps, as well as practical checklists that can be downloaded and filled-out to document validation.\n\nAt its core, the ValiText tool can be used for any of the following tasks:\n\n1.  Defining the key types of validation evidence that are required for sufficient validation\n2.  Guide researchers which concrete validation steps to apply for their text-based research, and\n3.  Provide a documentation template in the form of a checklist that can be used to document validation efforts effectively.\n\nIn this tutorial, we will begin by presenting the framework (Chapter 2), followed by a practical example of its use in measuring sexism in social media posts (Chapter 3), and finally conclude with a discussion (Chapter 4).\n\n# 2. ValiText Tool\n\n\nValidation is a critical task in text analysis and natural language processing. At its core, validation involves various activities to demonstrate that a method measures what it purports to measure [@cureton1951validity; @repke2024validity]. However, validating text-based measures can be challenging [@krippendorff2009content].\n\nTherefore, any empirical measure needs to be validated. One crucial problem in the validation of text-based measures, however, is the lack of conceptual clarity on how to conduct validation.\n\nTo provide practical guidance for researchers and users to conduct and communicate validation, ValiText offers a flexible and consistent approach to validation.\n\nAt its core, ValiText requires three types of validation evidence:\n\n- **Substantive Evidence**: Requires outlining the **theoretical underpinning** of the measure.\n- **Structural Evidence**: Requires examining and evaluating **properties of the model and its measures**.\n- **External Evidence**: Requires testing how the measure relates to **other independent information or criteria**.\n\nThe framework is complemented by a checklist that defines and outlines empirical validation steps available to collect validity evidence for different use cases.\n\nIf you want to learn more about the framework and the checklist, please click on the respective section below or have a look at our [Working Paper](https://arxiv.org/abs/2307.02863).\n\n\n::: panel-tabset\n### Framework\n\n\n![Conceptual Framework](images/framework.png)\n\n### Checklist\n\nDifferent checklists are available for different use cases, depending on the text-based methods used. The table below summarises the use cases, and provides download links to the checklists\n\n<div style=\"font-size: 0.85em;\">\n\n| #  | Use Case                                    | Training Data Required | Known Output Categories | Description                                                             | Example                                                                                                      | Checklist Download                                                                                                                                     |\n|----|---------------------------------------------|-------------------------------------|-------------------------|-------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n| A  | Dictionaries                                | No                                  | Yes                     | Assign scores to text units using predefined word lists                 | A dictionary assigns polarity values ranging from -1 to 1 to each known text unit                            | [![Download A](https://cdn-icons-png.flaticon.com/512/2739/2739765.png){width=30px}](https://github.com/lukasbirki/golemvalitext/raw/main/data-raw/checklists/checklist_A_ValiTex.docx){.btn}       |\n| B  | (Semi-) Supervised Classification           | Yes                                 | Yes                     | Train a model to predict known output categories based on labelled data  | A pretrained BERT model is fine-tuned on labelled social media posts and predicts “offensive” and “non-offensive” posts | [![Download B](https://cdn-icons-png.flaticon.com/512/2739/2739765.png){width=30px}](https://github.com/lukasbirki/golemvalitext/raw/main/data-raw/checklists/checklist_B_ValiTex.docx){.btn}       |\n| C  | Prompt-based Classification Using LLMs      | Yes                                 | Yes                     | Ask (“prompt”) LLMs to assign labels to texts                           | A Mistral model is prompted to determine whether a text is “sexist” or “non-sexist”                          | [![Download C](https://cdn-icons-png.flaticon.com/512/2739/2739765.png){width=30px}](https://github.com/lukasbirki/golemvalitext/raw/main/data-raw/checklists/checklist_C_ValiTex.docx){.btn}       |\n| D  | Topic Modelling                             | No                                  | No                      | Assign topics without any labeled data                                  | An LDA topic model generates 13 coherent topics                                                              | [![Download D](https://cdn-icons-png.flaticon.com/512/2739/2739765.png){width=30px}](https://github.com/lukasbirki/golemvalitext/raw/main/data-raw/checklists/checklist_D_ValiTex.docx){.btn}       |\n\n</div>\n\n\n:::\n\n# 3. Application and use case\n\nWe demonstrate the applicability of the tool by revisiting and documenting the validation steps from a study by @samory2021call. In their study, @samorey2021call rely on different types of supervised machine learning models to detect sexist social media posts within different social media datasets.\n\nMeasuring sexism is a challenging tasks, given that sexism is a complex phenomenon and can often be expressed in different nuances and subtle strategies. \nIn their paper, @samorey2021call thus explore different strategies to improve measurement of these data. Since they rely on a labeled dataset (i.e., data that contains already human-labaled annotations), they focus less on exploring substantive reserach questions, but rather how to improve the model classifiying the texts. \n\nGiven that they rely on supervised models, we select the template “B: (Semi-) Supervised Classification”. Ultimately, we demonstrate how the checklists offer structured guidance for researchers to navigate validation. On the other hand, we demonstrate how the tool can be used as a documentation scheme, streamlining communication between researchers engaged in validation and research consumers.\n\n\n\n## Substantive Evidence\n\n@samory2021call engage with the literature on sexism and computational methods, highlighting definitional ambiguities and potential biases. They justify their operationalization by developing a codebook based on subdimensions of sexism and annotating training data for machine-learning models, using supervised methods like Logit, CNN, and BERT to compare performance. The authors also discuss data collection decisions, use of adversarial examples, and select the sentence level for analysis, while providing preprocessing details primarily for the Logit model.\n\n\n::: {.callout-tip collapse=\"true\"}\n### Filled-Out Checklist \n\n<iframe src=\"images/substantive.pdf\" width=\"100%\" height=\"600px\"></iframe>\n\n\n:::\n\n---\n\n## Structural Evidence\n\n@samory2021call provide structural evidence by evaluating both model properties and output. They inspect predictive model features (II.1), comparing the most predictive unigrams across datasets and methods, noting that models trained on adversarial examples demonstrate greater robustness. For model output evaluation, they conduct error analysis (II.5) on misclassified examples using their BERT model, examining factors like model type, data origin, and coder agreement to identify systematic errors.\n\n::: {.callout-tip collapse=\"true\"}\n### Filled-Out Checklist \n\n<iframe src=\"images/structural.pdf\" width=\"100%\" height=\"600px\"></iframe>\n\n\n:::\n---\n\n## External Evidence\n\nTo demonstrate external evidence, @samory2021call primarily rely on the comparison of measures with a human-annotated test set (III.1). To calculate classification performance, they apply k-fold cross-validation and report F1 scores. The evaluation of F1 score is widely regarded as the most viable metric, as alternative metrics such as accuracy (i.e., the overall ratio of positive predictions) can be misleading when dealing with imbalanced data [@spelmen2018review]. \n\n::: {.callout-tip collapse=\"true\"}\n### Filled-Out Checklist \n\n<iframe src=\"images/external.pdf\" width=\"100%\" height=\"600px\"></iframe>\n\n\n:::\n\n# 4. Discussion\n\n<!-- Roughly summarised by ChatGPT -->\n\nIn summary, ValiText offers a structured and systematic framework for validating text-based measures of social constructs, such as sexism in online content. One of its major advantages is the clarity it brings to the validation process, enabling researchers to document and communicate their efforts effectively. By providing pre-structured checklists, ValiText reduces the cognitive load on researchers and promotes transparency, offering a uniform way to ensure that all critical aspects of validation—substantive, structural, and external—are addressed.\n\nHowever, despite its strengths, the framework may have some limitations. For example, the reliance on self-assessment tools like ValiText might introduce subjectivity into the process, as researchers could be inclined to interpret the validation steps in a way that supports their results. Furthermore, the tool assumes a baseline level of understanding of validation practices, which could be a barrier for researchers new to the field of computational text analysis.\n\nIn practice, the use of ValiText in studies like that of @samory2021call demonstrates the value of having a clear, consistent approach to validating computational methods. However, the dynamic nature of social constructs like sexism poses a unique challenge. Constructs often evolve over time and across contexts, which raises questions about the generalization of validation practices across different studies and datasets.\n\n","srcMarkdownNoYaml":"\n\n# 1. Introduction\n\nIn this tutorial, we introduce the [ValiText](https://www.valitex.info/) tool, a framework and instrument for validating text-based measures of social constructs.\nValiText helps researchers to execute and document validation by providing conceptual and practical guidance for researchers [@birkenmaier2023valitext].\n\nValidation is a necessary requirement for any text analysis [@grimmer2013text].\nEssentially, the purpose of validation is to ensure that what is to be measured (i.e., the numerical scores assigned to texts) correspond to the true^[Nevertheless, it’s important to acknowledge that the \"true\" nature of a text is inherently unobservable and can only be approximated. For instance, while we might interpret a certain text as positive or negative, these characteristics are not intrinsic to the text itself; they are inferred and open to subjective interpretation [@krippendorff2009content]] nature of the construct being studied.\n\nData Quality is an important requirement for validity. Low quality data can significantly affect the validity of text-based measures.\nFor instance, when the text data is incomplete, ambiguous, or misrepresentative, it becomes challenging to draw accurate inferences about social science phenomena. However, detecting these data quality issues can be difficult. ValiText helps researchers with a shared vocabulary denoting different validation steps, as well as practical checklists that can be downloaded and filled-out to document validation.\n\nAt its core, the ValiText tool can be used for any of the following tasks:\n\n1.  Defining the key types of validation evidence that are required for sufficient validation\n2.  Guide researchers which concrete validation steps to apply for their text-based research, and\n3.  Provide a documentation template in the form of a checklist that can be used to document validation efforts effectively.\n\nIn this tutorial, we will begin by presenting the framework (Chapter 2), followed by a practical example of its use in measuring sexism in social media posts (Chapter 3), and finally conclude with a discussion (Chapter 4).\n\n# 2. ValiText Tool\n\n\nValidation is a critical task in text analysis and natural language processing. At its core, validation involves various activities to demonstrate that a method measures what it purports to measure [@cureton1951validity; @repke2024validity]. However, validating text-based measures can be challenging [@krippendorff2009content].\n\nTherefore, any empirical measure needs to be validated. One crucial problem in the validation of text-based measures, however, is the lack of conceptual clarity on how to conduct validation.\n\nTo provide practical guidance for researchers and users to conduct and communicate validation, ValiText offers a flexible and consistent approach to validation.\n\nAt its core, ValiText requires three types of validation evidence:\n\n- **Substantive Evidence**: Requires outlining the **theoretical underpinning** of the measure.\n- **Structural Evidence**: Requires examining and evaluating **properties of the model and its measures**.\n- **External Evidence**: Requires testing how the measure relates to **other independent information or criteria**.\n\nThe framework is complemented by a checklist that defines and outlines empirical validation steps available to collect validity evidence for different use cases.\n\nIf you want to learn more about the framework and the checklist, please click on the respective section below or have a look at our [Working Paper](https://arxiv.org/abs/2307.02863).\n\n\n::: panel-tabset\n### Framework\n\n\n![Conceptual Framework](images/framework.png)\n\n### Checklist\n\nDifferent checklists are available for different use cases, depending on the text-based methods used. The table below summarises the use cases, and provides download links to the checklists\n\n<div style=\"font-size: 0.85em;\">\n\n| #  | Use Case                                    | Training Data Required | Known Output Categories | Description                                                             | Example                                                                                                      | Checklist Download                                                                                                                                     |\n|----|---------------------------------------------|-------------------------------------|-------------------------|-------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n| A  | Dictionaries                                | No                                  | Yes                     | Assign scores to text units using predefined word lists                 | A dictionary assigns polarity values ranging from -1 to 1 to each known text unit                            | [![Download A](https://cdn-icons-png.flaticon.com/512/2739/2739765.png){width=30px}](https://github.com/lukasbirki/golemvalitext/raw/main/data-raw/checklists/checklist_A_ValiTex.docx){.btn}       |\n| B  | (Semi-) Supervised Classification           | Yes                                 | Yes                     | Train a model to predict known output categories based on labelled data  | A pretrained BERT model is fine-tuned on labelled social media posts and predicts “offensive” and “non-offensive” posts | [![Download B](https://cdn-icons-png.flaticon.com/512/2739/2739765.png){width=30px}](https://github.com/lukasbirki/golemvalitext/raw/main/data-raw/checklists/checklist_B_ValiTex.docx){.btn}       |\n| C  | Prompt-based Classification Using LLMs      | Yes                                 | Yes                     | Ask (“prompt”) LLMs to assign labels to texts                           | A Mistral model is prompted to determine whether a text is “sexist” or “non-sexist”                          | [![Download C](https://cdn-icons-png.flaticon.com/512/2739/2739765.png){width=30px}](https://github.com/lukasbirki/golemvalitext/raw/main/data-raw/checklists/checklist_C_ValiTex.docx){.btn}       |\n| D  | Topic Modelling                             | No                                  | No                      | Assign topics without any labeled data                                  | An LDA topic model generates 13 coherent topics                                                              | [![Download D](https://cdn-icons-png.flaticon.com/512/2739/2739765.png){width=30px}](https://github.com/lukasbirki/golemvalitext/raw/main/data-raw/checklists/checklist_D_ValiTex.docx){.btn}       |\n\n</div>\n\n\n:::\n\n# 3. Application and use case\n\nWe demonstrate the applicability of the tool by revisiting and documenting the validation steps from a study by @samory2021call. In their study, @samorey2021call rely on different types of supervised machine learning models to detect sexist social media posts within different social media datasets.\n\nMeasuring sexism is a challenging tasks, given that sexism is a complex phenomenon and can often be expressed in different nuances and subtle strategies. \nIn their paper, @samorey2021call thus explore different strategies to improve measurement of these data. Since they rely on a labeled dataset (i.e., data that contains already human-labaled annotations), they focus less on exploring substantive reserach questions, but rather how to improve the model classifiying the texts. \n\nGiven that they rely on supervised models, we select the template “B: (Semi-) Supervised Classification”. Ultimately, we demonstrate how the checklists offer structured guidance for researchers to navigate validation. On the other hand, we demonstrate how the tool can be used as a documentation scheme, streamlining communication between researchers engaged in validation and research consumers.\n\n\n\n## Substantive Evidence\n\n@samory2021call engage with the literature on sexism and computational methods, highlighting definitional ambiguities and potential biases. They justify their operationalization by developing a codebook based on subdimensions of sexism and annotating training data for machine-learning models, using supervised methods like Logit, CNN, and BERT to compare performance. The authors also discuss data collection decisions, use of adversarial examples, and select the sentence level for analysis, while providing preprocessing details primarily for the Logit model.\n\n\n::: {.callout-tip collapse=\"true\"}\n### Filled-Out Checklist \n\n<iframe src=\"images/substantive.pdf\" width=\"100%\" height=\"600px\"></iframe>\n\n\n:::\n\n---\n\n## Structural Evidence\n\n@samory2021call provide structural evidence by evaluating both model properties and output. They inspect predictive model features (II.1), comparing the most predictive unigrams across datasets and methods, noting that models trained on adversarial examples demonstrate greater robustness. For model output evaluation, they conduct error analysis (II.5) on misclassified examples using their BERT model, examining factors like model type, data origin, and coder agreement to identify systematic errors.\n\n::: {.callout-tip collapse=\"true\"}\n### Filled-Out Checklist \n\n<iframe src=\"images/structural.pdf\" width=\"100%\" height=\"600px\"></iframe>\n\n\n:::\n---\n\n## External Evidence\n\nTo demonstrate external evidence, @samory2021call primarily rely on the comparison of measures with a human-annotated test set (III.1). To calculate classification performance, they apply k-fold cross-validation and report F1 scores. The evaluation of F1 score is widely regarded as the most viable metric, as alternative metrics such as accuracy (i.e., the overall ratio of positive predictions) can be misleading when dealing with imbalanced data [@spelmen2018review]. \n\n::: {.callout-tip collapse=\"true\"}\n### Filled-Out Checklist \n\n<iframe src=\"images/external.pdf\" width=\"100%\" height=\"600px\"></iframe>\n\n\n:::\n\n# 4. Discussion\n\n<!-- Roughly summarised by ChatGPT -->\n\nIn summary, ValiText offers a structured and systematic framework for validating text-based measures of social constructs, such as sexism in online content. One of its major advantages is the clarity it brings to the validation process, enabling researchers to document and communicate their efforts effectively. By providing pre-structured checklists, ValiText reduces the cognitive load on researchers and promotes transparency, offering a uniform way to ensure that all critical aspects of validation—substantive, structural, and external—are addressed.\n\nHowever, despite its strengths, the framework may have some limitations. For example, the reliance on self-assessment tools like ValiText might introduce subjectivity into the process, as researchers could be inclined to interpret the validation steps in a way that supports their results. Furthermore, the tool assumes a baseline level of understanding of validation practices, which could be a barrier for researchers new to the field of computational text analysis.\n\nIn practice, the use of ValiText in studies like that of @samory2021call demonstrates the value of having a clear, consistent approach to validating computational methods. However, the dynamic nature of social constructs like sexism poses a unique challenge. Constructs often evolve over time and across contexts, which raises questions about the generalization of validation practices across different studies and datasets.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"tool_valitext.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","editor":"visual","title":"ValiText - A Validation Framework for Text-Based Measures of Social Constructs","author":"Lukas Birkenmaier","bibliography":["references.bib"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":[]}